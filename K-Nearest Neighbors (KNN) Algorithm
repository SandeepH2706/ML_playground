K-Nearest Neighbors (KNN) Algorithm

K-Nearest Neighbors (KNN) is a simple, versatile, and widely used supervised machine learning algorithm used for both classification and regression problems. However, it is more commonly used in classification.
KNN is a non-parametric, instance-based (or lazy learning) algorithm, which means it doesn’t make assumptions about the underlying data distribution and doesn’t learn a discriminative function from the training data.
Instead, it memorizes the dataset and makes predictions based on the 'k' closest training examples in the feature space.

How It Works:
  Choose the number of neighbors k.
  Calculate the distance between the new input and all training samples (commonly Euclidean distance).
  Sort the distances and find the k nearest neighbors.
  For classification:
  Use majority voting among the k neighbors.
  For regression:
  Take the average (or weighted average) of the k neighbors’ values.

Key Parameters:
  n_neighbors: The number of neighbors to consider (k). Higher k reduces noise but may blur class boundaries.
  weights: 'uniform' (equal weight to all neighbors) or 'distance' (closer neighbors have more influence).
  algorithm: 'auto', 'ball_tree', 'kd_tree', or 'brute' — method used to compute nearest neighbors.
  p:  Power parameter for the Minkowski metric: p=1 → Manhattan, p=2 → Euclidean distance.
  metric:	The distance metric to use (default is 'minkowski'). Can be custom-defined.

Evaluating KNN Performance:
  Classification Metrics:
    Accuracy
    Precision, Recall, F1-score
    Confusion Matrix
    ROC-AUC Curve
  Regression Metrics:
    Mean Squared Error (MSE)
    Mean Absolute Error (MAE)
    R² Score (Coefficient of Determination)
Cross-validation
  K-Fold Cross-Validation is commonly used to choose the optimal value of k and avoid overfitting/underfitting.

Choosing the Best k:
  Small k → more sensitive to noise → high variance.
  Large k → smoother decision boundary → high bias.
  Use grid search with cross-validation to find the optimal k.

Limitations:
  Slow at prediction time for large datasets (since it scans all data)
  Sensitive to irrelevant or redundant features
  Performance degrades with high-dimensional data (curse of dimensionality)
  Requires feature scaling (e.g., StandardScaler or MinMaxScaler)
